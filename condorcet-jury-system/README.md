# Condorcet Jury System

**Synthetic user validation for product decisions using LLM-powered personas**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## What is this?

The Condorcet Jury System uses 1,000+ synthetic user personas to validate product decisions at every phase of development. Based on [Condorcet's Jury Theorem](https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem): if individual voters have >50% accuracy, the majority verdict approaches certainty as group size increases.

**Use cases:**
- Validate research findings resonate with target users
- Test PRD user stories and flows match mental models
- Evaluate prototypes for usability before real user testing
- Identify skeptic/edge-case concerns before launch

## Quick Start

### 1. Clone and install

```bash
git clone https://github.com/YOUR_USERNAME/condorcet-jury-system.git
cd condorcet-jury-system
pip install -r requirements.txt
```

### 2. Run your first evaluation (no API key needed)

```bash
python scripts/simulate_jury.py \
  --content "Your product idea or feature description here" \
  --jury-size 100
```

### 3. View results

Results are saved to `outputs/` with a verdict, breakdown by persona segment, and actionable recommendations.

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Your Product    â”‚â”€â”€â”€â”€â–¶â”‚ 1000 Synthetic  â”‚â”€â”€â”€â”€â–¶â”‚ Aggregated      â”‚
â”‚ Description     â”‚     â”‚ User Personas   â”‚     â”‚ Verdict         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼          â–¼          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Skeptic â”‚ â”‚ Early   â”‚ â”‚ Power   â”‚
              â”‚ 35%     â”‚ â”‚ Adopter â”‚ â”‚ User    â”‚
              â”‚         â”‚ â”‚ 20%     â”‚ â”‚ 24%     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Persona Distribution

| Archetype | % | Description |
|-----------|---|-------------|
| Sales Rep | 35% | Front-line users, time-pressed, quota-focused |
| Sales Leader | 25% | Strategic, team-focused, metrics-driven |
| CSM | 15% | Relationship-focused, customer advocates |
| Operations | 15% | Process-oriented, efficiency-focused |
| Strategic Consultant | 10% | Analytical, ROI-focused, skeptical |

### AI Adoption Stages (Critical for realistic feedback)

| Stage | % | Typical Approval Rate |
|-------|---|----------------------|
| Skeptic | 35% | ~25% (hardest to convince) |
| Curious | 21% | ~55% |
| Early-Adopter | 20% | ~75% |
| Power-User | 24% | ~85% |

## Commands

### Simulation Mode (Free, No API)

```bash
# Basic evaluation
python scripts/simulate_jury.py --content "Your feature idea"

# With initiative folder structure
python scripts/simulate_jury.py \
  --initiative path/to/initiative \
  --phase research \
  --jury-size 100

# Phases: research, prd, prototype
```

### API Mode (Requires Anthropic API Key)

```bash
export ANTHROPIC_API_KEY=your_key_here

python scripts/jury_evaluator.py \
  --initiative path/to/initiative \
  --phase research \
  --jury-size 100 \
  --skeptics 0.20
```

**Cost estimates:**
- 100 evaluations with Haiku: ~$0.50
- Synthesis with Sonnet: ~$0.10
- Full evaluation: ~$1-2 per initiative

### Generate Iteration Documents

After receiving jury feedback:

```bash
python scripts/iterate_from_feedback.py \
  --initiative path/to/initiative

# Generates:
# - research-gaps.md (interview questions for failing segments)
# - prd-amendments.md (requirements to add)
# - prototype-spec.md (UI components to build)
```

## Understanding Results

### Verdict Thresholds

| Combined Pass Rate | Verdict | Recommended Action |
|-------------------|---------|-------------------|
| â‰¥80% | STRONG PASS | Ship with confidence |
| â‰¥60% | PASS | Address key concerns, proceed |
| â‰¥40% | CONDITIONAL | Iterate before proceeding |
| <40% | FAIL | Major pivot needed |

### Example Output

```
ğŸ›ï¸  CONDORCET JURY EVALUATION
==================================================
Initiative: hubspot-agent-config-ui
Phase: research
Jury Size: 1000
==================================================

ğŸ“Š VERDICT SUMMARY
   âœ… Approve: 346 (34.6%)
   âš ï¸  Conditional: 160 (16.0%)
   âŒ Reject: 494 (49.4%)

ğŸ† OVERALL: FAIL (50.6% combined)

ğŸ“ˆ BY ADOPTION STAGE
   Skeptic: 18.2% pass (critical gap!)
   Curious: 52.9% pass
   Early-Adopter: 66.2% pass
   Power-User: 83.1% pass

ğŸ”¥ TOP FRICTION POINTS
   1. Rollback if something breaks (282 mentions)
   2. Integration with existing tools (274 mentions)
   3. Compliance/security concerns (250 mentions)
```

## Customizing Personas

### Edit Archetypes

Modify personas in `personas/archetypes/`:

```json
{
  "archetype": "sales-rep",
  "name": "Alex Chen",
  "bio": "Mid-level AE at a 200-person SaaS company...",
  "ai_adoption_stage": "curious",
  "painPoints": ["Tool fatigue", "Data entry time"],
  "voiceQuotes": ["I just want it to work without thinking"]
}
```

### Generate More Personas

```bash
python scripts/expand_personas.py \
  --count 1000 \
  --output personas/generated/
```

### Adjust Distribution

Edit `personas/generation-config.json`:

```json
{
  "archetypes": {
    "sales-rep": { "count": 350 },
    "sales-leader": { "count": 250 }
  },
  "attribute_distribution": {
    "ai_adoption_stage": {
      "skeptic": 0.35,
      "curious": 0.21,
      "early-adopter": 0.20,
      "power-user": 0.24
    }
  }
}
```

## Project Structure

```
condorcet-jury-system/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ simulate_jury.py       # Local simulation (no API)
â”‚   â”œâ”€â”€ jury_evaluator.py      # Full API evaluation
â”‚   â”œâ”€â”€ expand_personas.py     # Generate more personas
â”‚   â”œâ”€â”€ iterate_from_feedback.py # Generate iteration docs
â”‚   â””â”€â”€ generate_personas.py   # API-based persona generation
â”œâ”€â”€ personas/
â”‚   â”œâ”€â”€ archetypes/            # Master persona templates
â”‚   â”‚   â”œâ”€â”€ sales-rep.json
â”‚   â”‚   â”œâ”€â”€ sales-leader.json
â”‚   â”‚   â”œâ”€â”€ csm.json
â”‚   â”‚   â”œâ”€â”€ operations.json
â”‚   â”‚   â””â”€â”€ strategic-consultant.json
â”‚   â”œâ”€â”€ generated/             # Expanded persona pools
â”‚   â”œâ”€â”€ persona-schema.json    # JSON schema for validation
â”‚   â””â”€â”€ generation-config.json # Distribution config
â”œâ”€â”€ outputs/                   # Evaluation results
â””â”€â”€ examples/
    â””â”€â”€ sample-initiative/     # Example initiative structure
```

## Integrating with Your Workflow

### Cursor/AI IDE Integration

Add to your workspace rules:

```markdown
## Jury Validation Commands

| Command | Action |
|---------|--------|
| `eval [initiative]` | Run jury evaluation |
| `iterate [initiative]` | Generate iteration docs from feedback |
```

### CI/CD Integration

```yaml
# .github/workflows/jury-validation.yml
name: Jury Validation
on:
  pull_request:
    paths:
      - 'docs/prd/**'
      - 'docs/research/**'

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - run: python scripts/simulate_jury.py --initiative . --phase prd
      - name: Check verdict
        run: |
          PASS_RATE=$(cat outputs/latest-eval.json | jq '.combined_pass_rate')
          if (( $(echo "$PASS_RATE < 60" | bc -l) )); then
            echo "::error::Jury validation failed ($PASS_RATE% pass rate)"
            exit 1
          fi
```

## Theoretical Foundation

[Condorcet's Jury Theorem](https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem) states:

> If each voter has probability p > 0.5 of being correct, then P(majority correct) â†’ 1 as n â†’ âˆ

With n=100 jurors and p=0.65 individual accuracy:
- **P(majority correct) â‰ˆ 99.9%**

This makes synthetic user validation highly reliable when:
1. Personas accurately represent target users
2. Evaluation prompts elicit authentic responses
3. Jury size is sufficiently large (n â‰¥ 50)

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

- Based on research from [Condorcet's Jury Theorem applications to AI](https://arxiv.org/abs/2305.14325)
- Persona framework inspired by Jobs-to-be-Done methodology
- Built for product teams using AI-assisted development workflows

